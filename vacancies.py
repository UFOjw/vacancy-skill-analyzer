import requests
from bs4 import BeautifulSoup
from llama_cpp import Llama
import re
import pandas as pd
from tqdm import tqdm
import ast

llm = Llama(
    model_path="models/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
    n_ctx=6048,
    n_threads=6,
    n_gpu_layers=35,
    verbose=False,
)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å—Å—ã–ª–∫–µ
def extract_text_from_url(url: str) -> str:
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=1)

        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        # –£–¥–∞–ª–∏–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
        for tag in soup(['script', 'style']):
            tag.decompose()
        text = soup.get_text(separator=' ', strip=True)
        return text
    except Exception as e:
        print(f"[ERROR] Failed to process {url}: {e}")
        return ""

# –í—ã–¥–µ–ª–µ–Ω–∏–µ –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
def extract_topics(text: str) -> str:
    prompt = f"""[INST] –¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –≤–∞–∫–∞–Ω—Å–∏–π. –ù–∞ –≤—Ö–æ–¥ —Ç—ã –ø–æ–ª—É—á–∞–µ—à—å —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ –æ—Ç —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—è. –¢–≤–æ—è –∑–∞–¥–∞—á–∞:

–í—ã–¥–µ–ª–∏ —Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç –æ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞. –°—Ñ–æ—Ä–º–∏—Ä—É–π –∏—Ö **–≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É**, –ø–µ—Ä–µ—á–∏—Å–ª—è—è —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é.

–í–∫–ª—é—á–∏:
- —Å—Ç–µ–∫ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π: —è–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –±–∏–±–ª–∏–æ—Ç–µ–∫–∏, —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏, –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã;
- –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è: –æ–ø—ã—Ç –≤ –≥–æ–¥–∞—Ö, —É—Ä–æ–≤–µ–Ω—å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ, –∑–Ω–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–π, –¥–∏–ø–ª–æ–º, –æ–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π –∏ —Ç.–ø.

‚ö†Ô∏è –ù–µ –ø–∏—à–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π, –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, —Å–ø–∏—Å–∫–æ–≤, –∞–±–∑–∞—Ü–µ–≤ ‚Äî —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫—É —Å–æ —Å–∂–∞—Ç—ã–º–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏.

–û—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ –æ–±–µ—Ä–Ω–∏ –≤:
<answer>
[—Å—Ç—Ä–æ–∫–∞ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é]
</answer>

–¢–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏:
{text[:4000]} 
[/INST]"""
    result = llm(prompt, max_tokens=728, temperature=0.0, stop=["</s>"]) ### –ú–∞–∫—Å –∞—É—Ç–ø—É—Ç –º–æ–¥–µ–ª–∏ + —Ç–µ–∫—Å—Ç –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏
    return result["choices"][0]["text"].strip()

def extract_filtered_skills(text: str) -> list:
    prompt = f"""
–£ —Ç–µ–±—è –µ—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞–≤—ã–∫–æ–≤:
{categories}

–¢–µ–ø–µ—Ä—å –≤—ã–±–µ—Ä–∏ –∏–∑ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ø–∏—Å–∫–∞ **—Ç–æ–ª—å–∫–æ —Ç–µ –Ω–∞–≤—ã–∫–∏**, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ø–∞–¥–∞—é—Ç —Ö–æ—Ç—è –±—ã –≤ –æ–¥–Ω—É –∏–∑ —ç—Ç–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π:

Skills: {text}

–í–µ—Ä–Ω–∏ –æ—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ –≤ –≤–∏–¥–µ **—Å–ø–∏—Å–∫–∞ –Ω–∞–≤—ã–∫–æ–≤** —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, **–±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π –∏ –±–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π**. –ù–∞–ø—Ä–∏–º–µ—Ä:
Python, Git, Deep Learning, English

–ü—Ä–∞–≤–∏–ª–∞:
- –ù–µ –º–µ–Ω—è–π –Ω–∞–∑–≤–∞–Ω–∏—è –Ω–∞–≤—ã–∫–æ–≤.
- –ù–µ –≥—Ä—É–ø–ø–∏—Ä—É–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º.
- –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏–∫–∞–∫–æ–≥–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–∞–≤—ã–∫–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é.

–û—Ç–≤–µ—Ç:
"""
    result = llm(prompt, max_tokens=512, temperature=0.0, stop=["</s>"])
    skills_raw = result["choices"][0]["text"].strip()
    return [s.strip() for s in skills_raw.split(',') if s.strip()]

categories = """
–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–µ ML (—Ä–µ–≥—Ä–µ—Å—Å–∏—è, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è)
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç–æ–¥—ã ML: –±—É—Å—Ç–∏–Ω–≥, SVM, Random Forest, L1/L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã, –∞–Ω–∞–ª–∏–∑ —Å–∏–≥–Ω–∞–ª–æ–≤, –¥–µ—Ç–µ–∫—Ü–∏—è –≤—ã–±—Ä–æ—Å–æ–≤
Scikit-learn, CatBoost, XGBoost, LightGBM, GLM
Feature Engineering, Optuna, GridSearch, –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π: accuracy, precision, recall, F1, ROC-AUC
–ü—Ä–æ–¥–∞–∫—à–Ω –∏ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–æ–≤
ML-–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã: MLflow, DVC, ClearML, Triton Inference Server, ONNX
–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ª–æ–≥-–∞–Ω–∞–ª–∏–∑, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è

PyTorch, TensorFlow, Keras
LSTM, RNN, Transformers, BERT, GPT
Fine-tuning, LoRA, Prompt Tuning, Quantization
–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä, pruning, DDP/DeepSpeed/FSDP

LLM: GPT, Claude, LLaMA, Mistral
LangChain, AutoGEN, CrewAI, Hugging Face Transformers
Text Classification, NER, Summarization, RAG, Semantic Search
Sentence Transformers, spaCy, NLTK
–°—Ç—Ä—É–∫—Ç—É—Ä—ã: –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–Ω–∫–æ–¥–∏–Ω–≥–∏, attention, embedding models

OpenCV, torchvision, timm, Albumentations
YOLO, Detectron2, MMDetection
–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: img2img, IPAdapter, ControlNet, CFG
–ú–µ—Ç—Ä–∏–∫–∏: IoU, mAP, BLEU, ROUGE
CV –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: CVAT, Supervisely, Label Studio
Stable Diffusion, ComfyUI

A/B-—Ç–µ—Å—Ç—ã, –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø–æ—Ç–µ–∑, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–∏–ø–æ—Ç–µ–∑
–ú–µ—Ç—Ä–∏–∫–∏ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞: –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
Uplift modeling
BI-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã

SQL
ClickHouse, PostgreSQL, Oracle, GreenPlum, Hive, Spark, HDFS, Hadoop
Pandas, NumPy, PySpark
–†–∞–±–æ—Ç–∞ —Å —Å—ã—Ä—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏, –ª–æ–≥-–∞–Ω–∞–ª–∏–∑, –≥–µ–æ–¥–∞–Ω–Ω—ã–µ, –≥—Ä–∞—Ñ—ã

Docker, Kubernetes, Airflow, ML pipeline, CI/CD
MLOps: –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥, –∂–∏–∑–Ω–µ–Ω–Ω—ã–π —Ü–∏–∫–ª –º–æ–¥–µ–ª–µ–π, –ø—Ä–æ–¥–∞–∫—à–Ω-–ø–æ–¥–¥–µ—Ä–∂–∫–∞
DevOps: Kafka, TeamCity, Dagster, Nexus, GitHub, Bitbucket

FAISS, Chroma, Qdrant, Weaviate, Milvus
Vector Search, Recall@k, BM25
Knowledge Graph, embedding models (OpenAI, Hugging Face)

–¢–µ–æ—Ä–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π, –º–∞—Ç. —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, –º–∞—Ç. –∞–Ω–∞–ª–∏–∑, –ª–∏–Ω–µ–π–Ω–∞—è –∞–ª–≥–µ–±—Ä–∞
–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã
Terv–µr/matstat, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑

–û–ø—ã—Ç 1–≥–æ, –æ–ø—ã—Ç 3–≥–æ–¥–∞
–£—á–∞—Å—Ç–∏–µ –≤ Kaggle, –ø—É–±–ª–∏–∫–∞—Ü–∏–∏, GitHub-–ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ

–ü—Ä–æ—á–µ–µ
"""

urls = [
    ### –¥–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–∏ —Å–∞–π—Ç—ã
]

### –û–±—Ä–∞–±–æ—Ç—á–∏–∫
answers = []

for url in urls:
    print(f"\nüîó URL: {url}")
    text = extract_text_from_url(url)
    if text:
        output = extract_topics(text)
        print("üìå –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã:\n", output)

        match = re.search(r"<answer>(.*?)</answer>", output, re.DOTALL)
        if match:
            answer = match.group(1).strip()
            answers.append(answer)
            print("‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑ <answer>:\n", answer)
        else:
            print("‚ùå <answer> –Ω–µ –Ω–∞–π–¥–µ–Ω")

df = pd.read_excel("vac.xlsx") # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Ç–∞—Ä—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
answers_df = pd.DataFrame({"parsed": answers})

df = pd.concat([df, answers_df], axis=0).reset_index(drop=True)
df.to_excel("vac.xlsx", index=False)

"""
### –î–ª—è –ø–æ–∏—Å–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è categories)
combined_list = []

for line in df["parsed"]:
    tokens = [item.strip() for item in line.split(",")]
    combined_list.extend(tokens)

# –£–¥–∞–ª–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ:
combined_list = list(dict.fromkeys(combined_list))
print(combined_list)
"""

# –ü—Ä–æ–π—Ç–∏—Å—å –ø–æ –≤—Å–µ–º —Å—Ç—Ä–æ–∫–∞–º –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–≤—ã–∫–∏
df['filtered_skills'] = None

for idx, row in tqdm(df.iterrows(), total=len(df), desc="Filtering relevant skills"):
    skills_text = row['parsed']
    try:
        filtered = extract_filtered_skills(skills_text)
        df.at[idx, 'filtered_skills'] = filtered
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ —Å—Ç—Ä–æ–∫–µ {idx}: {e}")
        df.at[idx, 'filtered_skills'] = []

skills = {}
for row in df.iterrows():
    for skill in ast.literal_eval(row[1]['filtered_skills']):
        if skill not in skills:
            skills[skill] = 0
        skills[skill] += 1

print({k: v for k, v in sorted(skills.items(), key=lambda item: item[1])})